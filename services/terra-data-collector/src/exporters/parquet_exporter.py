"""
Parquet Exporter
ëŒ€ê·œëª¨ ML í•™ìŠµìš© Parquet ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°.
íš¨ìœ¨ì ì¸ ì»¬ëŸ¼ ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€, ë¹ ë¥¸ ë¶„ì„ ì¿¼ë¦¬.
"""
import logging
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd

from src.models import CollectedSensorData
from src.exporters import BaseExporter

logger = logging.getLogger(__name__)


class ParquetExporter(BaseExporter):
    """Parquet í˜•ì‹ìœ¼ë¡œ ëŒ€ê·œëª¨ ML í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""

    @property
    def name(self) -> str:
        return "parquet_export"

    async def export(self, records: List[CollectedSensorData]) -> int:
        """Parquet íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not records:
            return 0

        output_dir = Path(self.config.get("output_dir", "./data/exports/parquet"))
        output_dir.mkdir(parents=True, exist_ok=True)
        compression = self.config.get("compression", "snappy")

        rows = [r.to_training_record() for r in records]
        df = pd.DataFrame(rows)

        # ì„¼ì„œ íƒ€ì…ë³„ ë¶„í•  ì €ì¥ (ML í•™ìŠµ ì‹œ í¸ì˜)
        saved = 0
        for sensor_type, group_df in df.groupby("sensorType"):
            filepath = output_dir / f"sensor_{sensor_type}.parquet"

            if filepath.exists():
                existing = pd.read_parquet(filepath)
                group_df = pd.concat([existing, group_df], ignore_index=True)
                group_df.drop_duplicates(
                    subset=["sensorId", "timestamp"], keep="last", inplace=True,
                )

            group_df.to_parquet(filepath, compression=compression, index=False)
            saved += len(group_df)
            logger.debug(f"  ğŸ’¾ {filepath.name}: {len(group_df)} records")

        self._export_count += saved
        logger.info(f"ğŸ’¾ [parquet_export] Saved {saved} records to {output_dir}")
        return saved
