"""
Time Series Analyzer for Terra-Cortex
InfluxDB Flux ì¿¼ë¦¬ë¥¼ í†µí•œ ì„¼ì„œ ë°ì´í„° ì¶”ì„¸ ë¶„ì„

ê¸°ëŠ¥:
  1. ì´ë™ í‰ê·  (Moving Average) ì´ìƒ íƒì§€
  2. ìµœê·¼ Nì‹œê°„ ì¶”ì„¸ ë¶„ì„ (ìƒìŠ¹/í•˜ê°•/ì•ˆì •)
  3. ì¼/ì£¼/ì›” íŒ¨í„´ í†µê³„
  4. ê¸‰ë³€ ê°ì§€ (Rate of Change)
  5. ì˜ˆì¸¡ê°’ ì¶”ì • (ì„ í˜• íšŒê·€)

ì•„í‚¤í…ì²˜:
  - InfluxDB 2.7 Flux ì¿¼ë¦¬ ì§ì ‘ í˜¸ì¶œ
  - ê²°ê³¼ëŠ” TrendContextë¡œ íŒ¨í‚¤ì§• â†’ local_analyzer / cloud_advisorì— ì „ë‹¬
  - TTL ìºì‹œë¡œ ë™ì¼ farmId+sensorType ë°˜ë³µ ì¡°íšŒ ë°©ì§€
"""
import asyncio
import logging
import math
import os
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple

import httpx

logger = logging.getLogger(__name__)

# Configuration
INFLUXDB_URL = os.getenv("INFLUXDB_URL", "http://influxdb:8086")
INFLUXDB_TOKEN = os.getenv("INFLUXDB_TOKEN", "terra-token-2025")
INFLUXDB_ORG = os.getenv("INFLUXDB_ORG", "terraneuron")
INFLUXDB_BUCKET = os.getenv("INFLUXDB_BUCKET", "sensor_data")
TREND_CACHE_TTL = int(os.getenv("TREND_CACHE_TTL", "120"))  # 2ë¶„ (ì„¼ì„œ ë°ì´í„° ë¹ ë¦„)
TREND_WINDOW = os.getenv("TREND_WINDOW", "1h")  # ê¸°ë³¸ ë¶„ì„ ìœˆë„ìš°


@dataclass
class TrendPoint:
    """ì‹œê³„ì—´ ë°ì´í„° í¬ì¸íŠ¸"""
    time: datetime
    value: float


@dataclass
class TrendStats:
    """ì¶”ì„¸ í†µê³„ ìš”ì•½"""
    mean: float
    std: float
    min_val: float
    max_val: float
    count: int
    latest: float
    direction: str  # "rising", "falling", "stable"
    rate_of_change: float  # ë‹¨ìœ„ì‹œê°„ë‹¹ ë³€í™”ìœ¨
    moving_avg: float  # ì´ë™ í‰ê· 
    deviation_from_ma: float  # í˜„ì¬ê°’ - ì´ë™í‰ê·  (ì–‘ìˆ˜: í‰ê·  ì´ìƒ, ìŒìˆ˜: í‰ê·  ì´í•˜)
    is_spike: bool  # ê¸‰ë³€ ê°ì§€ (í˜„ì¬ê°’ì´ 2Ïƒ ì´ìƒ ë²—ì–´ë‚¨)
    predicted_next: Optional[float] = None  # ì„ í˜• íšŒê·€ ì˜ˆì¸¡ê°’ (10ë¶„ í›„)
    period: str = "1h"  # ë¶„ì„ ê¸°ê°„


@dataclass
class DailyPattern:
    """ì¼ê°„ íŒ¨í„´ í†µê³„"""
    hour: int
    avg_value: float
    min_value: float
    max_value: float


@dataclass
class TrendContext:
    """ë¶„ì„ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬ë˜ëŠ” ì¶”ì„¸ ì»¨í…ìŠ¤íŠ¸"""
    farm_id: str
    sensor_type: str
    has_data: bool
    stats: Optional[TrendStats] = None
    recent_points: List[TrendPoint] = field(default_factory=list)
    daily_pattern: List[DailyPattern] = field(default_factory=list)
    message: Optional[str] = None

    def to_context_string(self) -> str:
        """ë¶„ì„ íŒŒì´í”„ë¼ì¸ìš© ìš”ì•½ ë¬¸ìì—´"""
        if not self.has_data or not self.stats:
            return f"{self.farm_id}/{self.sensor_type}: ì‹œê³„ì—´ ë°ì´í„° ì—†ìŒ"

        s = self.stats
        parts = [
            f"ìµœê·¼ {s.period}: í‰ê· ={s.mean:.1f}, í‘œì¤€í¸ì°¨={s.std:.1f}",
            f"ë²”ìœ„={s.min_val:.1f}~{s.max_val:.1f}, ìµœì‹ ={s.latest:.1f}",
            f"ì¶”ì„¸={s.direction}(ë³€í™”ìœ¨={s.rate_of_change:+.2f}/h)",
            f"ì´ë™í‰ê· ={s.moving_avg:.1f}, í¸ì°¨={s.deviation_from_ma:+.1f}",
        ]
        if s.is_spike:
            parts.append("âš ï¸ ê¸‰ë³€ ê°ì§€")
        if s.predicted_next is not None:
            parts.append(f"10ë¶„í›„ ì˜ˆì¸¡={s.predicted_next:.1f}")
        return " | ".join(parts)


class TimeSeriesAnalyzer:
    """
    InfluxDB Flux ì¿¼ë¦¬ ê¸°ë°˜ ì‹œê³„ì—´ ë¶„ì„ê¸°
    """

    def __init__(self):
        self.url = INFLUXDB_URL
        self.token = INFLUXDB_TOKEN
        self.org = INFLUXDB_ORG
        self.bucket = INFLUXDB_BUCKET
        self.window = TREND_WINDOW
        self.cache_ttl = TREND_CACHE_TTL
        self.enabled = bool(self.token)

        # (farmId, sensorType) â†’ (TrendContext, fetch_time)
        self._cache: Dict[Tuple[str, str], Tuple[TrendContext, float]] = {}

        # í†µê³„
        self.query_count = 0
        self.cache_hit_count = 0
        self.error_count = 0

        logger.info(
            f"âœ… TimeSeriesAnalyzer initialized: "
            f"influxdb={self.url}, bucket={self.bucket}, window={self.window}, cache_ttl={self.cache_ttl}s"
        )

    def get_config(self) -> dict:
        """Return configuration and stats for /info endpoint"""
        return {
            "influxdb_url": self.url,
            "bucket": self.bucket,
            "org": self.org,
            "window": self.window,
            "cache_ttl": self.cache_ttl,
            "enabled": self.enabled,
            "stats": {
                "query_count": self.query_count,
                "cache_hit_count": self.cache_hit_count,
                "error_count": self.error_count,
            }
        }

    async def get_trend_context(
        self, farm_id: str, sensor_type: str, window: str = None
    ) -> Optional[TrendContext]:
        """
        íŠ¹ì • farmId/sensorTypeì˜ ìµœê·¼ ì¶”ì„¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ

        Args:
            farm_id: ë†ì¥ ì‹ë³„ì
            sensor_type: ì„¼ì„œ ìœ í˜• (temperature, humidity, co2, ...)
            window: ë¶„ì„ ìœˆë„ìš° (ê¸°ë³¸: 1h)

        Returns:
            TrendContext or None
        """
        w = window or self.window
        cache_key = (farm_id, sensor_type)

        # ìºì‹œ í™•ì¸
        now = time.time()
        if cache_key in self._cache:
            ctx, cached_at = self._cache[cache_key]
            if (now - cached_at) < self.cache_ttl:
                self.cache_hit_count += 1
                return ctx

        try:
            self.query_count += 1

            # InfluxDB Flux ì¿¼ë¦¬: ìµœê·¼ ìœˆë„ìš°ì˜ ë°ì´í„° ì¡°íšŒ
            points = await self._query_recent_data(farm_id, sensor_type, w)

            if not points or len(points) < 2:
                ctx = TrendContext(
                    farm_id=farm_id,
                    sensor_type=sensor_type,
                    has_data=False,
                    message=f"ë°ì´í„° ë¶€ì¡± (ìµœê·¼ {w}: {len(points) if points else 0}ê±´)",
                )
                self._cache[cache_key] = (ctx, now)
                return ctx

            # í†µê³„ ê³„ì‚°
            stats = self._compute_stats(points, w)

            # ì¼ê°„ íŒ¨í„´ (24ì‹œê°„ ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°)
            daily = []
            if w in ("24h", "1d", "7d", "30d") and len(points) > 24:
                daily = self._compute_daily_pattern(points)

            ctx = TrendContext(
                farm_id=farm_id,
                sensor_type=sensor_type,
                has_data=True,
                stats=stats,
                recent_points=points[-10:],  # ìµœê·¼ 10ê°œ í¬ì¸íŠ¸ë§Œ ë³´ì¡´
                daily_pattern=daily,
            )
            self._cache[cache_key] = (ctx, now)

            logger.info(
                f"ğŸ“ˆ Trend context: {farm_id}/{sensor_type} "
                f"â†’ {stats.direction} (Î¼={stats.mean:.1f}, Ïƒ={stats.std:.1f}, "
                f"spike={stats.is_spike})"
            )
            return ctx

        except Exception as e:
            self.error_count += 1
            logger.error(f"âŒ TimeSeries query error for {farm_id}/{sensor_type}: {e}")
            # ì´ì „ ìºì‹œê°€ ìˆìœ¼ë©´ ë§Œë£Œë˜ì–´ë„ ë°˜í™˜ (graceful degradation)
            if cache_key in self._cache:
                return self._cache[cache_key][0]
            return TrendContext(
                farm_id=farm_id,
                sensor_type=sensor_type,
                has_data=False,
                message=f"InfluxDB ì¿¼ë¦¬ ì˜¤ë¥˜: {str(e)[:100]}",
            )

    async def _query_recent_data(
        self, farm_id: str, sensor_type: str, window: str
    ) -> List[TrendPoint]:
        """InfluxDB Flux ì¿¼ë¦¬ë¡œ ìµœê·¼ ë°ì´í„° ì¡°íšŒ"""
        flux_query = f'''
from(bucket: "{self.bucket}")
  |> range(start: -{window})
  |> filter(fn: (r) => r["_measurement"] == "sensor_data")
  |> filter(fn: (r) => r["farmId"] == "{farm_id}")
  |> filter(fn: (r) => r["sensorType"] == "{sensor_type}")
  |> filter(fn: (r) => r["_field"] == "value")
  |> sort(columns: ["_time"])
'''
        headers = {
            "Authorization": f"Token {self.token}",
            "Content-Type": "application/vnd.flux",
            "Accept": "application/csv",
        }

        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                f"{self.url}/api/v2/query?org={self.org}",
                headers=headers,
                content=flux_query,
            )
            response.raise_for_status()

        return self._parse_csv_response(response.text)

    async def query_daily_stats(
        self, farm_id: str, sensor_type: str, days: int = 7
    ) -> List[Dict[str, Any]]:
        """ì¼ë³„ í†µê³„ (í‰ê· , ìµœì†Œ, ìµœëŒ€) â€” REST APIìš©"""
        flux_query = f'''
from(bucket: "{self.bucket}")
  |> range(start: -{days}d)
  |> filter(fn: (r) => r["_measurement"] == "sensor_data")
  |> filter(fn: (r) => r["farmId"] == "{farm_id}")
  |> filter(fn: (r) => r["sensorType"] == "{sensor_type}")
  |> filter(fn: (r) => r["_field"] == "value")
  |> aggregateWindow(every: 1d, fn: mean, createEmpty: false)
  |> yield(name: "daily_mean")
'''
        headers = {
            "Authorization": f"Token {self.token}",
            "Content-Type": "application/vnd.flux",
            "Accept": "application/csv",
        }

        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{self.url}/api/v2/query?org={self.org}",
                    headers=headers,
                    content=flux_query,
                )
                response.raise_for_status()

            points = self._parse_csv_response(response.text)
            return [
                {"date": p.time.strftime("%Y-%m-%d"), "mean": round(p.value, 2)}
                for p in points
            ]
        except Exception as e:
            logger.error(f"âŒ Daily stats query error: {e}")
            return []

    async def query_hourly_pattern(
        self, farm_id: str, sensor_type: str, days: int = 7
    ) -> List[Dict[str, Any]]:
        """ì‹œê°„ëŒ€ë³„ í‰ê·  íŒ¨í„´ (ìµœê·¼ Nì¼) â€” REST APIìš©"""
        flux_query = f'''
import "date"

from(bucket: "{self.bucket}")
  |> range(start: -{days}d)
  |> filter(fn: (r) => r["_measurement"] == "sensor_data")
  |> filter(fn: (r) => r["farmId"] == "{farm_id}")
  |> filter(fn: (r) => r["sensorType"] == "{sensor_type}")
  |> filter(fn: (r) => r["_field"] == "value")
  |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)
  |> map(fn: (r) => ({{ r with hour: date.hour(t: r._time) }}))
  |> group(columns: ["hour"])
  |> mean(column: "_value")
'''
        headers = {
            "Authorization": f"Token {self.token}",
            "Content-Type": "application/vnd.flux",
            "Accept": "application/csv",
        }

        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{self.url}/api/v2/query?org={self.org}",
                    headers=headers,
                    content=flux_query,
                )
                response.raise_for_status()

            points = self._parse_csv_response(response.text)
            return [
                {"hour": i, "avg": round(p.value, 2)}
                for i, p in enumerate(points)
            ]
        except Exception as e:
            logger.error(f"âŒ Hourly pattern query error: {e}")
            return []

    def _parse_csv_response(self, csv_text: str) -> List[TrendPoint]:
        """InfluxDB CSV ì‘ë‹µì„ TrendPoint ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±"""
        points = []
        lines = csv_text.strip().split("\n")
        if len(lines) < 2:
            return points

        # í—¤ë” íŒŒì‹±
        header = None
        for line in lines:
            if line.startswith("#") or line.strip() == "":
                continue
            if header is None:
                header = line.split(",")
                continue

            cols = line.split(",")
            if len(cols) < len(header):
                continue

            col_map = dict(zip(header, cols))

            # _time ê³¼ _value ì¶”ì¶œ
            time_str = col_map.get("_time", "")
            value_str = col_map.get("_value", "")

            if not time_str or not value_str:
                continue

            try:
                # ISO 8601 íŒŒì‹±
                t = datetime.fromisoformat(time_str.replace("Z", "+00:00"))
                v = float(value_str)
                points.append(TrendPoint(time=t, value=v))
            except (ValueError, TypeError):
                continue

        return points

    def _compute_stats(self, points: List[TrendPoint], window: str) -> TrendStats:
        """ì¶”ì„¸ í†µê³„ ê³„ì‚°"""
        values = [p.value for p in points]
        n = len(values)

        mean_val = sum(values) / n
        std_val = math.sqrt(sum((v - mean_val) ** 2 for v in values) / n) if n > 1 else 0.0
        min_val = min(values)
        max_val = max(values)
        latest = values[-1]

        # ì´ë™ í‰ê·  (ìµœê·¼ ì ˆë°˜)
        ma_window = max(n // 2, 3)
        ma_values = values[-ma_window:]
        moving_avg = sum(ma_values) / len(ma_values)

        # í˜„ì¬ê°’ê³¼ ì´ë™í‰ê· ì˜ í¸ì°¨
        deviation = latest - moving_avg

        # ê¸‰ë³€ ê°ì§€: 2Ïƒ ì´ìƒ ë²—ì–´ë‚¨
        is_spike = abs(deviation) > 2 * std_val if std_val > 0 else False

        # ì¶”ì„¸ ë°©í–¥: ì „ë°˜ë¶€ í‰ê·  vs í›„ë°˜ë¶€ í‰ê· 
        half = n // 2
        first_half_mean = sum(values[:half]) / half if half > 0 else mean_val
        second_half_mean = sum(values[half:]) / (n - half) if (n - half) > 0 else mean_val
        diff = second_half_mean - first_half_mean

        if abs(diff) < std_val * 0.3:
            direction = "stable"
        elif diff > 0:
            direction = "rising"
        else:
            direction = "falling"

        # ë³€í™”ìœ¨: (ë§ˆì§€ë§‰ - ì²˜ìŒ) / ì‹œê°„ì°¨(ì‹œê°„)
        time_span_hours = (
            (points[-1].time - points[0].time).total_seconds() / 3600.0
        )
        rate_of_change = (
            (values[-1] - values[0]) / time_span_hours
            if time_span_hours > 0
            else 0.0
        )

        # ì„ í˜• íšŒê·€ ì˜ˆì¸¡ (10ë¶„ í›„)
        predicted = self._linear_predict(points, minutes_ahead=10)

        return TrendStats(
            mean=round(mean_val, 2),
            std=round(std_val, 2),
            min_val=round(min_val, 2),
            max_val=round(max_val, 2),
            count=n,
            latest=round(latest, 2),
            direction=direction,
            rate_of_change=round(rate_of_change, 3),
            moving_avg=round(moving_avg, 2),
            deviation_from_ma=round(deviation, 2),
            is_spike=is_spike,
            predicted_next=round(predicted, 2) if predicted is not None else None,
            period=window,
        )

    def _linear_predict(
        self, points: List[TrendPoint], minutes_ahead: int = 10
    ) -> Optional[float]:
        """ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¡œ Në¶„ í›„ ì˜ˆì¸¡"""
        if len(points) < 5:
            return None

        # ìµœê·¼ ë°ì´í„°ë¡œë§Œ íšŒê·€ (ìµœëŒ€ 30ê°œ)
        recent = points[-30:]
        n = len(recent)
        base_time = recent[0].time.timestamp()

        xs = [(p.time.timestamp() - base_time) / 60.0 for p in recent]  # ë¶„ ë‹¨ìœ„
        ys = [p.value for p in recent]

        x_mean = sum(xs) / n
        y_mean = sum(ys) / n

        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(xs, ys))
        denominator = sum((x - x_mean) ** 2 for x in xs)

        if denominator == 0:
            return None

        slope = numerator / denominator
        intercept = y_mean - slope * x_mean

        # Në¶„ í›„ ì˜ˆì¸¡
        target_x = xs[-1] + minutes_ahead
        return slope * target_x + intercept

    def _compute_daily_pattern(self, points: List[TrendPoint]) -> List[DailyPattern]:
        """ì‹œê°„ëŒ€ë³„ í†µê³„ ê³„ì‚°"""
        hour_data: Dict[int, List[float]] = {}
        for p in points:
            h = p.time.hour
            hour_data.setdefault(h, []).append(p.value)

        patterns = []
        for h in sorted(hour_data.keys()):
            vals = hour_data[h]
            patterns.append(
                DailyPattern(
                    hour=h,
                    avg_value=round(sum(vals) / len(vals), 2),
                    min_value=round(min(vals), 2),
                    max_value=round(max(vals), 2),
                )
            )
        return patterns

    def get_stats(self) -> Dict[str, Any]:
        return {
            "provider": "influxdb",
            "url": self.url,
            "bucket": self.bucket,
            "window": self.window,
            "enabled": self.enabled,
            "total_queries": self.query_count,
            "cache_hits": self.cache_hit_count,
            "errors": self.error_count,
            "cached_keys": len(self._cache),
        }


# Singleton factory
_analyzer: Optional[TimeSeriesAnalyzer] = None


def get_timeseries_analyzer() -> TimeSeriesAnalyzer:
    """ì‹±ê¸€í†¤ TimeSeriesAnalyzer ë°˜í™˜"""
    global _analyzer
    if _analyzer is None:
        _analyzer = TimeSeriesAnalyzer()
    return _analyzer
