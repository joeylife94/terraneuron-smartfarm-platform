# RAG Implementation - Quick Start Guide

## ðŸŽ¯ Overview

This RAG (Retrieval-Augmented Generation) implementation enhances terra-cortex AI recommendations with agricultural domain knowledge from PDF documents.

**Before RAG:**
- Generic LLM advice: "Check irrigation system"

**After RAG:**
- Expert advice: "According to GlobalG.A.P. standards, soil moisture below 40% requires emergency irrigation within 1 hour to prevent blossom-end rot. Check drip lines and emitters immediately."

## ðŸ“‹ Prerequisites

- Docker and docker-compose installed
- Ollama running on host machine (port 11434)
- Terra-cortex service configured with volume mapping

## ðŸš€ Quick Start (5 minutes)

### Step 1: Add PDF Documents

```bash
# Place agricultural PDFs in knowledge base directory
cd services/terra-cortex/data/knowledge_base

# For testing, we've provided sample_agricultural_guide.txt
# Convert it to PDF or add your own PDFs
```

**Recommended Documents:**
- GlobalG.A.P. certification manuals
- Crop-specific cultivation guides
- Pest & disease management handbooks
- Climate control best practices

### Step 2: Rebuild Terra-Cortex (Install RAG Dependencies)

```bash
# Stop existing container
docker-compose stop terra-cortex

# Rebuild with new requirements (langchain, chromadb, sentence-transformers)
docker-compose build terra-cortex

# Start container
docker-compose up -d terra-cortex
```

**Build Time:** 3-5 minutes (downloads embedding model: all-MiniLM-L6-v2)

### Step 3: Ingest Knowledge Base

```bash
# Run ingestion script inside container
docker exec -it terra-cortex python -m src.ingest_knowledge
```

**Expected Output:**
```
============================================================
ðŸŒ¾ Starting Knowledge Base Ingestion Pipeline
============================================================
ðŸ“š Found 1 PDF files
   ðŸ“„ Loading: sample_agricultural_guide.pdf
      âœ… Loaded 12 pages
âœ… Total pages loaded: 12
ðŸ”„ Chunking 12 documents...
âœ… Created 156 chunks (chunk_size=1000, overlap=200)
ðŸ”„ Creating ChromaDB vector database...
âœ… ChromaDB vector database created and persisted
============================================================
âœ… Knowledge Base Ingested: 12 documents processed
   ðŸ“š Total chunks: 156
   ðŸ’¾ Vector DB: /app/data/chroma_db
============================================================
```

**Ingestion Time:** 1-3 minutes (depends on PDF size)

### Step 4: Restart Terra-Cortex (Load RAG)

```bash
# Restart to load RAG advisor
docker-compose restart terra-cortex

# Verify RAG is enabled
docker logs terra-cortex --tail 30
```

**Look for these log entries:**
```
âœ… RAG Advisor enabled: 156 documents
âœ… Cloud Advisor enabled with LOCAL LLM: llama3.1
ðŸ” RAG search: 'soilMoisture anomaly 7.97 agricultural...'
âœ… Retrieved 3 context chunks
ðŸ¤– Requesting RAG-enhanced LLM recommendation...
```

### Step 5: Test RAG Recommendations

```bash
# Run simulation with mixed mode (generates anomalies)
python tests/simulation.py --mode mixed --count 10 --report
```

**Check HTML Report:**
- Open generated `test_report_YYYYMMDD_HHMMSS.html`
- Look at **AI Recommendation** column
- Anomalies should show detailed, context-aware advice

## ðŸ” How RAG Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sensor Anomaly  â”‚ Temperature: 35Â°C (Critical High)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG Retrieval (ChromaDB)                    â”‚
â”‚ Query: "temperature anomaly 35 agricultural"â”‚
â”‚ â†“                                            â”‚
â”‚ Top 3 Relevant Chunks:                      â”‚
â”‚ 1. "Heat stress >32Â°C: ventilation + shade" â”‚
â”‚ 2. "Tomato critical high: 32Â°C threshold"   â”‚
â”‚ 3. "Emergency cooling: misting system"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Enhanced LLM Prompt                          â”‚
â”‚ "Based on agricultural best practices:      â”‚
â”‚  [Context 1] [Context 2] [Context 3]        â”‚
â”‚  Current: Temperature 35Â°C ANOMALY          â”‚
â”‚  Question: What immediate action?"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama LLM (llama3.1)                        â”‚
â”‚ Generates context-aware recommendation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Deploy 50% shade cloth immediately and     â”‚
â”‚  activate evaporative cooling. According to â”‚
â”‚  best practices, temperatures above 32Â°C    â”‚
â”‚  cause blossom drop - monitor plants every  â”‚
â”‚  2 hours for heat stress symptoms."         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“Š Verification Checklist

âœ… **Dependencies Installed**
```bash
docker exec -it terra-cortex pip list | grep -E "langchain|chromadb|sentence"
```
Should show:
- chromadb==0.4.22
- langchain==0.1.0
- langchain-community==0.0.10
- sentence-transformers==2.2.2

âœ… **Knowledge Base Ingested**
```bash
ls -lh services/terra-cortex/data/chroma_db/
```
Should contain ChromaDB files (~10-50 MB)

âœ… **RAG Enabled in Logs**
```bash
docker logs terra-cortex 2>&1 | grep "RAG"
```
Should show:
```
âœ… RAG Advisor enabled: 156 documents
ðŸ” RAG search: ...
âœ… Retrieved 3 context chunks
```

âœ… **API Health Check**
```bash
curl http://localhost:8082/health
```
Should include:
```json
{
  "rag_advisor": {
    "enabled": true,
    "document_count": 156
  }
}
```

## ðŸ”§ Troubleshooting

### Problem: "No PDF files found"
**Solution:**
```bash
# Add PDF to knowledge_base directory
cp /path/to/manual.pdf services/terra-cortex/data/knowledge_base/

# Or convert sample TXT to PDF
# Then re-run ingestion
docker exec -it terra-cortex python -m src.ingest_knowledge
```

### Problem: "RAG Advisor disabled"
**Cause:** ChromaDB not found or empty

**Solution:**
```bash
# Check if chroma_db exists
ls services/terra-cortex/data/chroma_db/

# If empty, run ingestion
docker exec -it terra-cortex python -m src.ingest_knowledge

# Restart terra-cortex
docker-compose restart terra-cortex
```

### Problem: "Import langchain could not be resolved"
**Cause:** Requirements not installed

**Solution:**
```bash
# Rebuild container (installs requirements.txt)
docker-compose build terra-cortex
docker-compose up -d terra-cortex
```

### Problem: Slow ingestion (>5 minutes)
**Cause:** Large PDFs or many documents

**Normal Behavior:**
- Embeddings computed on CPU (no GPU required)
- ~10-30 seconds per PDF page
- 100-page PDF = 15-50 minutes

**Optimization:**
- Pre-filter PDFs (remove non-relevant pages)
- Use smaller chunk_size (500 instead of 1000)
- Run ingestion once, reuse ChromaDB across restarts

### Problem: "sentence-transformers model not found"
**Cause:** Model download failed during build

**Solution:**
```bash
# Manually download model inside container
docker exec -it terra-cortex python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Or rebuild Dockerfile (includes pre-download step)
docker-compose build --no-cache terra-cortex
```

## ðŸ“ˆ Performance Impact

### Memory Usage
- **Embedding Model**: ~120 MB (all-MiniLM-L6-v2)
- **ChromaDB**: ~50-200 MB (depends on document count)
- **Total Increase**: ~200-400 MB

### Response Time
- **Without RAG**: ~300-500 ms (LLM only)
- **With RAG**: ~800-1200 ms (retrieval + LLM)
- **Additional Latency**: ~300-700 ms for similarity search

### Recommendation Quality
- **Without RAG**: Generic, sometimes inaccurate
- **With RAG**: Domain-specific, cites best practices, actionable

**Example Comparison:**

| Scenario | Without RAG | With RAG |
|----------|-------------|----------|
| Soil Moisture 7% | "Water immediately" | "Emergency irrigation within 1 hour per GlobalG.A.P. Check drip emitters. Moisture <40% risks blossom-end rot." |
| Temperature 35Â°C | "Increase cooling" | "Deploy 50% shade cloth + misting. Heat stress >32Â°C causes blossom drop - monitor every 2 hours per protocol." |
| CO2 1900 ppm | "Reduce CO2" | "Evacuate workers if >3000 ppm. Open all vents, stop CO2 injection. Levels >1500 ppm risk toxicity per safety guidelines." |

## ðŸ”„ Updating Knowledge Base

### Adding New PDFs
```bash
# 1. Add PDF to knowledge_base
cp new_manual.pdf services/terra-cortex/data/knowledge_base/

# 2. Re-run ingestion (incremental - only processes new files)
docker exec -it terra-cortex python -m src.ingest_knowledge

# 3. Restart terra-cortex (reload vector DB)
docker-compose restart terra-cortex
```

### Replacing All Documents
```bash
# 1. Remove old ChromaDB
rm -rf services/terra-cortex/data/chroma_db/*

# 2. Replace PDFs in knowledge_base
rm services/terra-cortex/data/knowledge_base/*.pdf
cp new_pdfs/*.pdf services/terra-cortex/data/knowledge_base/

# 3. Re-ingest from scratch
docker exec -it terra-cortex python -m src.ingest_knowledge

# 4. Restart
docker-compose restart terra-cortex
```

## ðŸŽ“ Next Steps

1. **Add Domain PDFs**
   - GlobalG.A.P. v5.4 certification manual
   - Crop-specific guides (tomato, lettuce, pepper)
   - Local agricultural extension service publications

2. **Tune RAG Parameters**
   - Edit `src/rag_advisor.py`: `top_k=3` â†’ `top_k=5` (more context)
   - Edit `src/ingest_knowledge.py`: `chunk_size=1000` â†’ `chunk_size=500` (finer granularity)

3. **Monitor Quality**
   - Compare RAG vs non-RAG recommendations
   - Collect farmer feedback on actionability
   - Iterate on document selection and chunking strategy

4. **Scale Up**
   - Add more PDFs (up to 1000 pages supported)
   - Consider GPU acceleration for faster embedding (NVIDIA Docker runtime)
   - Implement periodic re-ingestion (cron job)

## ðŸ“š File Structure

```
services/terra-cortex/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest_knowledge.py      # PDF ingestion script (NEW)
â”‚   â”œâ”€â”€ rag_advisor.py            # RAG retrieval system (NEW)
â”‚   â”œâ”€â”€ cloud_advisor.py          # LLM integration (UPDATED)
â”‚   â””â”€â”€ main.py                   # FastAPI app
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ knowledge_base/           # Add PDFs here (NEW)
â”‚   â”‚   â””â”€â”€ sample_agricultural_guide.txt
â”‚   â”œâ”€â”€ chroma_db/                # Vector DB (auto-generated) (NEW)
â”‚   â””â”€â”€ README.md                 # RAG documentation (NEW)
â”œâ”€â”€ requirements.txt              # Added RAG dependencies (UPDATED)
â”œâ”€â”€ Dockerfile                    # Pre-download embedding model (UPDATED)
â””â”€â”€ RAG_QUICKSTART.md             # This file (NEW)
```

## âœ… Success Criteria

Your RAG implementation is working when:

1. âœ… Ingestion completes without errors
2. âœ… Logs show "RAG Advisor enabled: X documents"
3. âœ… Logs show "Retrieved 3 context chunks" for anomalies
4. âœ… HTML report shows detailed, context-aware AI recommendations
5. âœ… Recommendations cite specific best practices (e.g., "per GlobalG.A.P.")

**RAG transforms your AI from generic chatbot to domain expert!** ðŸŒ¾ðŸ§ 
